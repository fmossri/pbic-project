[system]
log_level = "INFO"
log_file = "logs/app.log"
storage_base_path = "storage/domains"
# default_domain = "" # Opcional: Definir um nome de domínio padrão

[ingestion]
# Estratégia de chunking: "recursive" ou potencialmente "semantic" depois
chunk_strategy = "recursive"
# Parâmetros para RecursiveCharacterTextSplitter
chunk_size = 1000
chunk_overlap = 200

[embedding]
# Nome do modelo Hugging Face para sentence-transformers. default: "sentence-transformers/all-MiniLM-L6-v2"
model_name = "sentence-transformers/all-MiniLM-L6-v2"
# Dispositivo: "cpu" ou "cuda" (certifique-se de que faiss-gpu está instalado para cuda). default: "cpu"
# Considerar adicionar lógica para detectar cuda automaticamente se disponível
device = "cpu"
# Tamanho do lote para geração de embeddings. default: 32
batch_size = 32
# Normalizar embeddings (recomendado para similaridade de cosseno). default: true
normalize_embeddings = true

[vector_store]
# Tipo de índice FAISS: "IndexFlatL2" (simples, busca exata), potencialmente "IndexIDMap" depois
index_type = "IndexFlatL2"
# Parâmetros para tipos específicos de índice (ex: nlist para IndexIVFFlat) iriam aqui
# index_params = { nlist = 100 } # Exemplo

[query]
# Número de chunks relevantes a recuperar do FAISS
retrieval_k = 5
# Futuro: Estratégia de re-ranking (ex: "none", "cohere", "cross-encoder")
# rerank_strategy = "none"

[llm]
# ID do repositório do modelo Hugging Face Hub para inferência
# Default: "mistralai/Mistral-7B-Instruct-v0.1"
model_repo_id = "mistralai/Mistral-7B-Instruct-v0.1"

# --- Parâmetros da Geração de Texto --- 

# Máximo de tokens a serem gerados na resposta. Controla o comprimento máximo.
# Default: 1000
max_new_tokens = 1000

# Controla a aleatoriedade: valores baixos (<0.5) tornam a saída mais focada/determinística,
# valores altos (>0.7) tornam mais aleatória/criativa.
# Default: 0.7
temperature = 0.7

# Nucleus sampling: considera apenas os tokens cuja probabilidade acumulada excede top_p.
# Alternativa/Complemento à temperature para controlar aleatoriedade.
# Default: 0.9
top_p = 0.9

# Considera apenas os 'k' tokens mais prováveis para a próxima palavra.
# Limita o pool de palavras candidatas.
# Default: 50
top_k = 50

# Penaliza tokens que apareceram recentemente para reduzir repetições.
# Valores > 1.0 aplicam penalidade. 1.0 = sem penalidade.
# Default: 1.1
repetition_penalty = 1.1

# --- Parâmetros de Retry --- 
# Número máximo de tentativas de chamada à API em caso de falha.
# Default: 3
max_retries = 3

# Tempo de espera (em segundos) entre as tentativas de chamada à API.
# Default: 2
retry_delay_seconds = 2

# --- Template do Prompt --- 
# Define a estrutura do prompt enviado ao LLM, incluindo contexto e query.
# Default: (Ver modelo LLMConfig em models.py) !!! Não remover Context:{context} Question: {query} Helpful Answer:
# Usar um motor de template como Jinja2 depois se necessário
prompt_template = """Use o seguinte contexto para responder a pergunta no final.
Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.
Mantenha a resposta concisa e diretamente ao ponto da pergunta.
Forneça a resposta *apenas* com base no contexto fornecido. Não adicione informações externas.

Context:
{context}

Question: {query}
Helpful Answer:""" 