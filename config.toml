[system]
storage_base_path = "storage"
control_db_filename = "control.db"

[ingestion]
# Estratégia de chunking: "recursive" ou potencialmente "semantic" depois
chunk_strategy = "recursive"

# --- Parâmetros para RecursiveCharacterTextSplitter ---

# Tamanho máximo em caracteres do chunk.
# Default: 1000
chunk_size = 1000

# Tamanho do overlap entre os chunks. Controla quantos caracteres do final do chunk anterior estarão presentes no ínicio do próximo chunk.
# Default: 200
chunk_overlap = 200

[embedding]
# Nome do modelo Hugging Face para sentence-transformers. default: "sentence-transformers/all-MiniLM-L6-v2"
model_name = "sentence-transformers/all-MiniLM-L6-v2"

# Dispositivo usado pelo sentence-transformers para gerar embeddings. opções: "cpu" ou "cuda". default: "cpu"
# Considerar adicionar lógica para detectar cuda automaticamente se disponível
device = "cpu"

# Tamanho do lote para geração de embeddings. default: 32
batch_size = 32

# Normalizar embeddings (recomendado para similaridade de cosseno). default: true
normalize_embeddings = true

[vector_store]
# Tipo de índice FAISS: "IndexFlatL2" (simples, busca exata), potencialmente "IndexIDMap" depois
index_type = "IndexFlatL2"
# Parâmetros para tipos específicos de índice (ex: nlist para IndexIVFFlat) iriam aqui
# index_params = { nlist = 100 }
# Dispositivo usado pelo FAISS para criar/buscar nos vector_stores. (não implementado)
# device = "cpu"

[query]
# Número de chunks relevantes a recuperar do FAISS
retrieval_k = 5
# Futuro: Estratégia de re-ranking (ex: "none", "cohere", "cross-encoder")
# rerank_strategy = "none"

[llm]
# ID do repositório do modelo Hugging Face Hub para inferência
# Default: "HuggingFaceH4/zephyr-7b-beta"
model_repo_id = "HuggingFaceH4/zephyr-7b-beta"

# --- Parâmetros da Geração de Texto --- 

# Máximo de tokens a serem gerados na resposta. Controla o comprimento máximo.
# Default: 1000
max_new_tokens = 1000

# Controla a aleatoriedade: valores baixos (<0.5) tornam a saída mais focada/determinística,
# valores altos (>0.7) tornam mais aleatória/criativa.
# Default: 0.7
temperature = 0.7

# Nucleus sampling: considera apenas os tokens cuja probabilidade acumulada excede top_p.
# Alternativa/Complemento à temperature para controlar aleatoriedade.
# Default: 0.9
top_p = 0.9

# Considera apenas os 'k' tokens mais prováveis para a próxima palavra.
# Limita o pool de palavras candidatas.
# Default: 50
top_k = 50

# Penaliza tokens que apareceram recentemente para reduzir repetições.
# Valores > 1.0 aplicam penalidade. 1.0 = sem penalidade.
# Default: 1.1
repetition_penalty = 1.1

# --- Parâmetros de Retry --- 
# Número máximo de tentativas de chamada à API em caso de falha.
# Default: 3
max_retries = 3

# Tempo de espera (em segundos) entre as tentativas de chamada à API.
# Default: 2
retry_delay_seconds = 2

# --- Template do Prompt --- 
# Define a estrutura do prompt enviado ao LLM, incluindo contexto e query.
# Default: (Ver modelo LLMConfig em models.py)
prompt_template = """Use o seguinte contexto para responder a pergunta no final.
Se você não sabe a resposta, apenas diga que não sabe, não tente inventar uma resposta.
Mantenha a resposta concisa e diretamente ao ponto da pergunta.
Forneça a resposta *apenas* com base no contexto fornecido. Não adicione informações externas."""

[text_normalizer]
# Configurações para os passos de normalização de texto

# Aplicar normalização Unicode (NFKC) (ex: "\u00e6" (æ) -> "ae")
use_unicode_normalization = true

# Converter todo o texto para minúsculas?
use_lowercase = true

# Remover caracteres especiais (mantendo letras, números, espaços, '.,-)?
use_remove_special_chars = true

# Remover espaços em branco extras (no início/fim e múltiplos espaços)?
use_remove_extra_whitespace = true 